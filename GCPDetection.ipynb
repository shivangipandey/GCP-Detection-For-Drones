{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\shivangi pandey\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import time \n",
    "from matplotlib import pyplot as plt\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crop the round bounded rectangle by rotating its axis to align it's axis with the global one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_minAreaRect(image, rect):\n",
    "    \n",
    "    centre, dimensions, theta = rect\n",
    "    \n",
    "    width = int(dimensions[0])+4\n",
    "    height = int(dimensions[1])+4\n",
    "    \n",
    "    box = cv2.boxPoints(rect)\n",
    "    box = np.int0(box)\n",
    "    \n",
    "    M = cv2.moments(box)\n",
    "    \n",
    "    if(M['m00'] == 0):\n",
    "        return (False,None)\n",
    "    \n",
    "    cx = int(M['m10']/M['m00'])\n",
    "    cy = int(M['m01']/M['m00'])\n",
    "\n",
    "    center = (cx,cy)\n",
    "    if theta < -45:\n",
    "        theta = theta + 90\n",
    "        width, height = height, width\n",
    "        \n",
    "    theta *= math.pi / 180 # convert to rad\n",
    "    v_x = (math.cos(theta), math.sin(theta))\n",
    "    v_y = (-math.sin(theta), math.cos(theta))\n",
    "    s_x = center[0] - v_x[0] * (width / 2) - v_y[0] * (height / 2)\n",
    "    s_y = center[1] - v_x[1] * (width / 2) - v_y[1] * (height / 2)\n",
    "    mapping = np.array([[v_x[0],v_y[0], s_x], [v_x[1],v_y[1], s_y]])\n",
    "\n",
    "    return (True,cv2.warpAffine(image, mapping, (width, height), flags=cv2.WARP_INVERSE_MAP, borderMode=cv2.BORDER_REPLICATE))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use saved model (previously trained model with accuracy 89% in DetectionModel) to predict the GCPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictValue(img,model):\n",
    "    \n",
    "    if(model == None):\n",
    "        return -1\n",
    "        \n",
    "    # expand (10,10) to (1,10,10)\n",
    "    img = (np.expand_dims(img,0))\n",
    "    predictions_single = model.predict(img)\n",
    "    \n",
    "    return np.argmax(predictions_single[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect which of the Countours are valid contour, that satisfy the property of our GCP\n",
    "## To cover wide range, we are considering polygon vertices between 2-6 (should be 4 in practical)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectValid(cont, ero, oriIm):\n",
    "   \n",
    "    #get the rectangle bounded by the contour (rotated rectatngle)\n",
    "    rect = cv2.minAreaRect(cont)\n",
    "    #Crop the rounded rectangle as an image\n",
    "    resu = crop_minAreaRect(ero, rect)\n",
    "    \n",
    "    if(not resu[0]):\n",
    "        return (False,None,None)\n",
    "    \n",
    "    im = resu[1]\n",
    "    im = cv2.bitwise_not(im)\n",
    "    \n",
    "    _, Cs, _ = cv2.findContours(im,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    img2 = oriIm.copy()\n",
    "    \n",
    "    m = 0\n",
    "    maxC = None\n",
    "    \n",
    "    for c in Cs:    \n",
    "        if(c is None or len(c) == 0):\n",
    "            continue\n",
    "        peri = cv2.arcLength(c, True)\n",
    "        hull = cv2.convexHull(c)\n",
    "        app = cv2.approxPolyDP(hull, 0.04 * peri, True)\n",
    "        \n",
    "        if(len(app) >= 2 and len(app) <= 6):\n",
    "            Ar = cv2.contourArea(app)\n",
    "            if(Ar >= m):\n",
    "                maxC = app\n",
    "                m = Ar\n",
    "                \n",
    "    if(maxC is None):\n",
    "        return (False,maxC,resu[1])\n",
    "    return (True,maxC,resu[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here Histogram is used to get optimum tresholding value. \n",
    "## For this, I'm considering 10th brightest pixel present in the grayscale image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findGCPPixelShade(histogram):\n",
    "    i = 255\n",
    "    count = 0;\n",
    "    while(i>=150):\n",
    "        if histogram[i,0] != 0:\n",
    "            count += 1\n",
    "            if count == 5:\n",
    "                return i\n",
    "        i -= 1\n",
    "    return i   \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Following processes are taking place, here\n",
    "## - Resize image, convert into grayscale,\n",
    "## - apply bilateral filter for blurring, adaptive thresholding, 2 iterations of dilation and 1 time erosion\n",
    "## - Extract contours with area less than 50 and a valid contour by using above method\n",
    "## - Create a binary image by thresholding value obtained from histogram of the image\n",
    "## - Count the no. of white pixels in to contour region in the binary image, if it's not 0, than pass the above cropped image to the trained model, to classify weather it's GCP or not\n",
    "## - model will return 1 if GCP, 0 otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractContours(imPath,resize,name,model):\n",
    "    image = cv2.imread(imPath)\n",
    "    small = cv2.resize(image,resize)\n",
    "    \n",
    "    #Convert into gray scale\n",
    "    gray_image = cv2.cvtColor(small, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # create the histogram\n",
    "    histogram = cv2.calcHist([gray_image], [0], None, [256], [0, 256])\n",
    "    index = findGCPPixelShade(histogram)\n",
    "    \n",
    "    ret,binImg = cv2.threshold(gray_image,index,255,cv2.THRESH_BINARY)\n",
    "    \n",
    "    #smoothning filter\n",
    "    blur2 = cv2.bilateralFilter(gray_image,15,10,10)\n",
    "    \n",
    "    th2 = cv2.adaptiveThreshold(blur2,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY,73,-40)\n",
    "    \n",
    "    kernel = np.ones((1,1),np.uint8)\n",
    "    dilation = cv2.dilate(th2,kernel,iterations = 2)\n",
    "    erosion = cv2.erode(dilation,kernel,iterations = 1)\n",
    "    \n",
    "    im2, contours, hierarchy = cv2.findContours(erosion,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    img = small.copy()\n",
    "    \n",
    "    # for each contour\n",
    "    max = 0\n",
    "    con = []\n",
    "    k = 0\n",
    "    for cnt in contours:\n",
    "        area = cv2.contourArea(cnt)    \n",
    "        if(area <=50):\n",
    "            epsilon = 0.005*cv2.arcLength(cnt,True)\n",
    "            approx = cv2.approxPolyDP(cnt,epsilon,True)\n",
    "            x,y,w,h = cv2.boundingRect(approx)\n",
    "            if(approx is None):\n",
    "                continue\n",
    "            v = math.fabs(w-h)\n",
    "            \n",
    "            if(v <= 10):\n",
    "                area = cv2.contourArea(approx)\n",
    "                app = detectValid(approx, erosion, img)\n",
    "                \n",
    "                if(not app[0]):\n",
    "                    continue  \n",
    "           \n",
    "                if(app[2].shape[0] > 20 or app[2].shape[1] > 20):    \n",
    "                    continue\n",
    "                area2 = cv2.contourArea(app[1])\n",
    "                areaDif = math.fabs(area2 - area)\n",
    "                \n",
    "                a1 = cv2.countNonZero(binImg[y:y+h+4,x:x+w+4])\n",
    "                if(a1 != 0):\n",
    "                    modelImg = cv2.resize(app[2],(10,10))\n",
    "                    print('*',end='-')\n",
    "                    if(predictValue(modelImg,model) == 1):\n",
    "                        con.append(approx)\n",
    "                    else:\n",
    "                        img = cv2.rectangle(img,(x-2,y-2),(x+w+4,y+h+4),(0,0,255),1)\n",
    "    \n",
    "    #Return (x,y) points and the image with valid rectangles\n",
    "    points = []\n",
    "    for c in con:\n",
    "        x,y,w,h = cv2.boundingRect(c)\n",
    "        points.append((x,y))\n",
    "        img = cv2.rectangle(img,(x-2,y-2),(x+w+4,y+h+4),(255,0,0),1)\n",
    "    \n",
    "    return (len(con),img,points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An image is returned after each iternation with red rectangles for values = 0 (not GCP according to the model) and Blue for GCP classified value by the model\n",
    "\n",
    "## All the rectangles are identified using above image processing methods, which further are sent to machine learning model for further classification\n",
    "\n",
    "## Current accuracy is 89%, which can be imporved significantly with more data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalResult(path):\n",
    "    model = load_model('GCP_Detection_Model.h5')\n",
    "    for i,ls in enumerate(os.listdir(path)):\n",
    "        \n",
    "        print(\"\\nfor image : \"+ls+\"\\n\")\n",
    "     \n",
    "        result =  extractContours(path+\"/\"+ls,(1366,768),ls,model)\n",
    "    \n",
    "        if(result[0] == 0):\n",
    "            print(\"No marker detected!\")\n",
    "        else:\n",
    "            print(\"\\n(x,y) : \",result[2])\n",
    "        \n",
    "        cv2.imshow(ls, result[1])\n",
    "        cv2.waitKey()\n",
    "        cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\shivangi pandey\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\models.py:282: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "for image : DJI_0000_set2 (105).JPG\n",
      "\n",
      "*-*-*-*-"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-1f162aec4334>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfinalResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./AssignmentDataset'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-75bac72d8568>\u001b[0m in \u001b[0;36mfinalResult\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nfor image : \"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mls\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mextractContours\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"/\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mls\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1366\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m768\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mls\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-e7415858eecd>\u001b[0m in \u001b[0;36mextractContours\u001b[1;34m(imPath, resize, name, model)\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m                 \u001b[0marea\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontourArea\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mapprox\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m                 \u001b[0mapp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdetectValid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mapprox\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merosion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m                 \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0mapp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-4e250107b299>\u001b[0m in \u001b[0;36mdetectValid\u001b[1;34m(cont, ero, oriIm)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindContours\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRETR_EXTERNAL\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCHAIN_APPROX_SIMPLE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mimg2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moriIm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "finalResult('./AssignmentDataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
